# üìò Lesson 6 ‚Äì Classification

‚úÖ In **Lesson 4**, we explored **Regression** ‚Äî predicting continuous values such as house prices.
‚úÖ We saw how models like **Linear Regression** and **Polynomial Regression** work, and how to measure their performance with metrics like MAE, RMSE, and R¬≤.
‚úÖ In **Lesson 5**, we built a **House Price Prediction project** ‚Äî regression in action ‚Äî where we used **Linear Regression** and **Random Forest** to make real predictions.

üëâ **Now in Lesson 6, we focus on Classification ‚Äî another core supervised learning method.**

---

## üéØ Learning Objectives

By the end of this lesson, students will be able to:

* Understand what **Classification** is and how it differs from Regression.
* Learn about common **Classification algorithms**:

  * Logistic Regression
  * Decision Trees
  * Random Forest
* Understand how to **evaluate classification models** using metrics:

  * Accuracy
  * Precision
  * Recall
  * F1-Score
  * Confusion Matrix

---

## 1) Introduction

Classification is one of the most widely used areas in Machine Learning.
üëâ Instead of predicting a number (like house price), we predict a **category** (spam vs not spam, churn vs not churn, healthy vs sick).

---

## 2) What is Classification?

**Definition:**

* **Classification** is a type of supervised learning where the target variable is **categorical** (e.g., Yes/No, Male/Female, Pass/Fail).
* The model learns from labeled data and predicts which class a new observation belongs to.

---

### Real-World Examples

* üìß **Spam Detection**: Predict whether an email is spam or not.
* üì± **Customer Churn**: Predict whether a customer will leave a service or stay.
* üê∂ **Image Recognition**: Predict if a picture is of a cat, dog, or another animal.

---

### Contrast with Regression

* **Regression:** Predicts **continuous values** (e.g., house price = \$350,000).
* **Classification:** Predicts **categories** (e.g., spam vs not spam).

üìå **Key Point:**
Regression = predicting **how much**.
Classification = predicting **which class**.

---

## 3) Classification Algorithms

### a) Logistic Regression

* Despite the name, it‚Äôs used for **classification**, not regression.
* Uses the **sigmoid function** to map predictions to probabilities (0‚Äì1).
* Example: Predicting whether a student passes an exam (Yes/No).

---

### b) Decision Trees

* Models decisions as a **tree structure** (if/else rules).
* Easy to interpret, but can **overfit** if not pruned.
* Example: Predict whether a loan is approved based on income, credit score, and history.

---

### c) Random Forest

* An **ensemble of many decision trees** working together.
* More accurate and robust than a single tree.
* Example: Predicting customer churn more reliably than a single tree.

üìå **Important Note:**
Classification algorithms are **not limited** to just Logistic Regression, Decision Trees, and Random Forest.
There are many more (like **Naive Bayes, KNN, Support Vector Machines, Neural Networks**).
üëâ But the key takeaway is to **understand how classification works in general** ‚Äî once you know that, you can research and choose the algorithm that fits your problem best.

---

## 4) Classification Metrics

When we build a classification model, we need to evaluate how good it is. ---

---

### üéØ 1) Accuracy

> Of **all predictions made**, how many were **correct** (both positives and negatives)?

* üìå Focuses on the **overall correctness** of the model.

* üí° Use when **classes are balanced** (almost equal positives and negatives).
  ‚ö†Ô∏è Not reliable when data is **imbalanced** (e.g. 95% healthy, 5% sick).

**Example**

* Model predicted 100 emails:

  * 90 predictions are correct (either spam correctly or not spam correctly)
* **Accuracy = 90 / 100 = 0.9 (90%)**

---

### üéØ 2) Precision

> Of all the cases the model **predicted as positive**, how many were **actually positive**?

* üìå Focuses on **quality of positive predictions**.

* üí° Use when **false positives are dangerous**, e.g.

  * Saying a healthy person has cancer
  * Accusing an innocent email of being spam

**Example**

* Model predicted 10 emails as spam ‚Üí 7 are really spam
* **Precision = 7 / 10 = 0.7 (70%)**

---

### üéØ 3) Recall (Sensitivity)

> Of all the **actual positive cases**, how many did the model **successfully detect**?

* üìå Focuses on **catching all positives**.

* üí° Use when **false negatives are dangerous**, e.g.

  * Missing a real cancer patient
  * Letting real spam into the inbox

**Example**

* There are 10 real spam emails ‚Üí model caught 7
* **Recall = 7 / 10 = 0.7 (70%)**

---

### ‚öñÔ∏è 4) F1-Score

> A **balanced measure** of Precision and Recall.

* üìå Useful when classes are **imbalanced**.

* üí° High F1 means the model is **both accurate and thorough** at finding positives.

**Example**

* Precision = 0.7, Recall = 0.7
* **F1 = 0.7**
* If one is very low, F1 will also be low.

---

### üìã 5) Confusion Matrix

> A table that shows **the complete picture** of predictions

|                     | **Predicted Positive** | **Predicted Negative** |
| ------------------- | ---------------------- | ---------------------- |
| **Actual Positive** | True Positive (TP) ‚úÖ   | False Negative (FN) ‚ùå  |
| **Actual Negative** | False Positive (FP) ‚ùå  | True Negative (TN) ‚úÖ   |

* Shows **exactly what errors** the model makes
* Lets you calculate **all other metrics** from it

---

### üß† Quick Summary Table

| Metric           | Question it answers                                     | Focus                    |
| ---------------- | ------------------------------------------------------- | ------------------------ |
| Accuracy         | How many predictions are correct overall?               | Overall correctness      |
| Precision        | Of predicted positives, how many are actually positive? | Avoiding false positives |
| Recall           | Of actual positives, how many are caught?               | Avoiding false negatives |
| F1-Score         | How well balance precision & recall?                    | Balanced performance     |
| Confusion Matrix | What are the counts of TP, FP, FN, TN?                  | Complete error breakdown |

---

## ‚úÖ Lesson Summary

* **Classification** is a supervised ML task where the goal is to predict **categories**.
* Common algorithms: **Logistic Regression, Decision Trees, Random Forest**.
* But remember: there are **many more algorithms** (KNN, Naive Bayes, SVM, Neural Networks) ‚Äî the most important skill is knowing **how classification works** and learning how to select the best algorithm for your problem.
* Evaluation metrics: **Accuracy, Precision, Recall, F1-Score, Confusion Matrix**.
* Key difference:

  * Regression ‚Üí predicts **numbers**.
  * Classification ‚Üí predicts **classes**.

---

## üîö End of Lesson 6

üéâ Congratulations! You‚Äôve completed **Classification ‚Äì your second core supervised learning method in Machine Learning.**

---
