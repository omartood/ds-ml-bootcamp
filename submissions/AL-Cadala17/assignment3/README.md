# 🛠️ Assignment 3: Data Preprocessing Pipeline

### 📋 Overview
This folder contains the deliverables for a data preprocessing assignment. The task involved transforming a raw, messy dataset on car prices into a clean, machine-learning-ready format by handling common data issues such as missing values, duplicates, and outliers.

### 🎯 Key Objectives
The core objectives of this assignment were to implement a robust and reproducible data preprocessing pipeline by performing the following steps:

* **Data Inspection**: 🔍 Load and perform an initial inspection to identify key data issues.
* **Target Cleaning**: 🧹 Clean and format the target variable (`Price`) for numerical analysis.
* **Data Imputation**: 💧 Handle missing values using appropriate strategies for numerical and categorical features.
* **Duplicate Removal**: 🗑️ Identify and remove duplicate rows to ensure data integrity.
* **Outlier Handling**: 📈 Manage extreme values using the IQR capping method.
* **Feature Transformation**: ✨ One-hot encode categorical features and engineer new features to improve model performance.
* **Feature Scaling**: ⚖️ Standardize continuous features, ensuring the target variable is not scaled to prevent data leakage.
* **Finalization**: ✅ Perform final checks and save the preprocessed data to a new CSV file.

### 📂 Included Files
This assignment includes the following files:

* `car_l3_dataset.csv`: The original, raw dataset used for this assignment.
* `l3_preprocess.py`: A Python script that implements all the data preprocessing steps from start to finish.
* `car_l3_clean_ready.csv`: The final, cleaned and preprocessed dataset generated by the script.
* `reflection.md`: A markdown document explaining the key decisions and reasoning behind the major steps in the preprocessing pipeline.
* `requirements.txt`: A file listing the exact Python packages and their versions required to run the script.