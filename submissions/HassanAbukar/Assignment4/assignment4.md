# ğŸ“˜ Lesson 4 â€” Regression

## ğŸ“Œ Definition:
- A supervised learning method used when the target (Y) is a **continuous numeric value**.
- The model predicts a number within a range.

### ğŸ” Examples:
- Predicting house prices: $150,000 â†’ $200,000  
- Predicting car value: $5,000 â†’ $20,000  
- Predicting temperature: 27.5Â°C  

### ğŸ› ï¸ Use Cases:
- Car Price Prediction (your dataset)  
- Sales forecasting  
- Weather prediction  

---

## ğŸ†š Classification

### ğŸ“Œ Definition:
- A supervised learning method used when the target (Y) is a **category (class)**.
- The model predicts **discrete labels**, like Yes/No or multiple classes.

### ğŸ” Examples:
- Does this person have a disease? â†’ Yes (1) / No (0)  
- Is this email spam? â†’ Spam / Not Spam  
- Animal type â†’ Cat / Dog / Bird  

### ğŸ› ï¸ Use Cases:
- Love recommender â†’ Is this person a match? Yes/No  
- Sentiment analysis â†’ Positive/Negative  
- Fraud detection â†’ Fraud / Legit  

---

## ğŸ¯ Key Difference

| Task           | Output Type        | Example                        |
|----------------|--------------------|--------------------------------|
| Regression     | Continuous Value   | Car price = $180,000          |
| Classification | Discrete Category  | This email is Spam            |

---

## ğŸ“Š Main Types of Regression

### 1. **Simple Linear Regression**
- **Definition:** Predicts the target using one independent variable with a straight-line relationship.
- **Example:** Predict car price based only on CarAge.
- **Strength:** Easy to understand, quick to implement.
- **Limitation:** Only works when the relationship is roughly linear and based on one feature.

### 2. **Multiple Linear Regression**
- **Definition:** Predicts the target using two or more independent variables.
- **Example:** Predict car price using CarAge + Km_per_year + Is_FamilyCar.
- **Strength:** Considers many factors at once.
- **Limitation:** Assumes linearity; sensitive to multicollinearity (when features are highly correlated).

### 3. **Polynomial Regression**
- **Definition:** Extends linear regression by adding polynomial terms (squared, cubic) to capture curves.
- **Example:** Predict housing price based on square footage when the relationship curves upward.
- **Strength:** Captures non-linear trends.
- **Limitation:** Can overfit if degree is too high; harder to interpret.

| **Regression Type** | **Definition**                       | **Example**                             | **Strengths**                        | **Limitations**                                              |
|---------------------|--------------------------------------|-----------------------------------------|-------------------------------------|---------------------------------------------------------------|
| Simple Linear        | One feature with a straight line     | Car price vs CarAge                     | Very easy, interpretable            | Too simple, only one factor, assumes linearity                |
| Multiple Linear      | Many features, still straight line   | Car price vs CarAge + Km_per_year       | Considers multiple factors          | Assumes linearity, sensitive to correlated features           |
| Polynomial           | Adds squared/cubic terms for curves  | House price vs size (curved trend)      | Captures non-linear patterns        | Risk of overfitting, less interpretable                       |


---

## ğŸ“ Regression Metrics

### 1. **MAE â€“ Mean Absolute Error**
- **Definition:** Average of the absolute differences between predicted values and actual values.
- âœ… Easy to understand (real units)  
- âŒ Treats all errors equally; doesnâ€™t punish big mistakes extra.

### 2. **MSE â€“ Mean Squared Error**
- **Definition:** Average of squared differences.
- âœ… Penalizes large mistakes  
- âŒ Units are squared â†’ harder to interpret

### 3. **RMSE â€“ Root Mean Squared Error**
- **Definition:** Square root of MSE.
- âœ… Punishes large errors, keeps units  
- âŒ Still sensitive to outliers

### 4. **RÂ² â€“ Coefficient of Determination**
- **Definition:** How much of the variance in the target is explained by the model.
- âœ… Easy to compare models (higher = better)  
- âŒ Can be misleading if data is non-linear or has outliers

| **Metric** | **Measures**                | **Strength**                        | **Limitation**                            |
|------------|-----------------------------|-------------------------------------|-------------------------------------------|
| **MAE**    | Average absolute error      | Simple, interpretable               | Doesnâ€™t punish big errors                 |
| **MSE**    | Average squared error       | Punishes large errors               | Units squared, hard to read               |
| **RMSE**   | Root of MSE                 | Same units, punishes big errors     | Sensitive to outliers                     |
| **RÂ²**     | % variance explained        | Intuitive, easy to compare          | Misleading with bad models                |

---

## ğŸ“‰ Underfitting

- **Definition:** Model is too simple; fails to capture patterns in the data  
- **Performance:** Poor on both training and test data  
- **Example:** Linear regression on curved data  
- **Causes:**
  - Too simple model
  - Too few features
  - Too much regularization  
- **Prevention:**
  - Use more complex models (Polynomial, Random Forest)
  - Add more features
  - Reduce regularization

---

## ğŸ“ˆ Overfitting

- **Definition:** Model is too complex; memorizes training data instead of generalizing  
- **Performance:** Great on training, bad on test  
- **Example:** Polynomial regression (degree 15) perfectly fitting all points  
- **Causes:**
  - Too complex model
  - Too little training data
  - Noisy data  
- **Prevention:**
  - Simplify model
  - Use regularization (Ridge, Lasso)
  - Use more data
  - Cross-validation or Early Stopping (for neural nets)

---

## ğŸ˜ Good Fit

- **Definition:** Model captures the true patterns without being too simple or too complex  
- **Example:** Predicting house prices with features like size, location, rooms  
- **Signs:**
  - Low training error
  - Validation/test error is also low
  - Generalizes well  
- **Causes of Good Fit:**
  - Right model complexity
  - Good features & preprocessing
  - Enough data  
- **Strategy:**
  - Use train/test split
  - Use cross-validation
  - Regularize appropriately
  - Collect more data if needed

> **In short:**
> - Underfit = Dumb model ğŸ¤¦  
> - Overfit = Overly clever, memorizing model ğŸ¤¯  
> - Good Fit = Smart model that generalizes ğŸ˜

---

## ğŸŒ Case Study: Predicting Air Pollution (PM2.5)

### ğŸ¯ Title:  
**Predicting daily PM2.5 air pollution levels in Beijing**

### ğŸ¥… Goal:
- Build a regression model to predict PM2.5 concentration (Âµg/mÂ³) using weather & seasonal features.
- Support policymakers with early warnings on hazardous air days.

### ğŸ“Š Data:
- ~43,800 rows (hourly data, 2010â€“2014)
- Features:
  - year, month, day, hour  
  - DEWP â†’ Dew point temperature (Â°C)  
  - TEMP â†’ Temperature (Â°C)  
  - PRES â†’ Atmospheric pressure (hPa)  
  - WSPM â†’ Wind speed (m/s)  
  - cbwd â†’ Combined wind direction (categorical)  
  - **Target:** PM2.5  

### ğŸ¤– Models Used:
| Model                     | RMSE (Âµg/mÂ³) | RÂ²     |
|--------------------------|--------------|--------|
| Multiple Linear Regression | ~38         | 0.65   |
| Random Forest Regression   | ~25         | 0.82   |
| XGBoost Regression         | ~21         | 0.89   |

> ğŸ† **Best Model:** XGBoost Regression

### ğŸ’¡ Key Insights:
1. Multiple Linear Regression was too simple â†’ underfitting seasonal effects  
2. Random Forest improved results by capturing non-linearities  
3. XGBoost combined many weak models â†’ strongest generalization  
4. Pollution spikes are influenced by weather & seasonality (esp. winter heating)

---

## ğŸ“š References

- **Dataset:** Beijing PM2.5 Data Set  
- **Source:** Beijing Municipal Environmental Monitoring Center  
- **Hosted at:** UCI Machine Learning Repository  
- **Link:** [UCI â€“ Beijing PM2.5 Data Set](https://archive.ics.uci.edu/ml/datasets/Beijing+PM2.5+Data)

